{"SLAM": [{"id": "2305.01599", "title": "EgoLocate: Real-time Motion Capture, Localization, and Mapping with Sparse Body-mounted Sensors", "url": "http://arxiv.org/pdf/2305.01599.pdf", "time": "2023-05-02", "code": "https://xinyu-yi.github.io/EgoLocate/", "context": ["EgoLocate is a system that integrates inertial sensors and cameras for real-time human motion capture, localization, and mapping. It uses image-based SLAM for localization and inertial mocap for camera motion prior, improving localization accuracy compared to state of the art techniques. The code is available for research at https://xinyu-yi.github.io/EgoLocate/.", "EgoLocate\u7cfb\u7edf\u5c06\u60ef\u6027\u4f20\u611f\u5668\u548c\u76f8\u673a\u96c6\u6210\u5728\u4e00\u8d77\uff0c\u7528\u4e8e\u5b9e\u65f6\u4eba\u4f53\u8fd0\u52a8\u6355\u6349\u3001\u5b9a\u4f4d\u548c\u5efa\u56fe\u3002\u5b83\u4f7f\u7528\u57fa\u4e8e\u56fe\u50cf\u7684SLAM\u8fdb\u884c\u5b9a\u4f4d\uff0c\u4f7f\u7528\u60ef\u6027mocap\u63d0\u4f9b\u76f8\u673a\u8fd0\u52a8\u5148\u9a8c\uff0c\u76f8\u6bd4\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u63d0\u9ad8\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728https://xinyu-yi.github.io/EgoLocate/\u4e0a\u83b7\u53d6\u3002"]}, {"id": "2305.00406", "title": "LIMOT: A Tightly-Coupled System for LiDAR-Inertial Odometry and Multi-Object Tracking", "url": "http://arxiv.org/pdf/2305.00406.pdf", "time": "2023-04-30", "code": null, "context": ["This study proposes LIMOT, a tightly-coupled multi-object tracking and LiDAR-inertial SLAM system for autonomous driving. LIMOT uses 3D bounding boxes generated by an object detector and performs LiDAR odometry using IMU pre-integration. Object association is performed based on historical trajectories of tracked objects. A trajectory-based dynamic feature filtering method is used to filter out features belonging to moving objects. Factor graph-based optimization is then conducted to optimize IMU bias and pose estimation. Experiments show that LIMOT achieves better pose and tracking accuracy than baseline methods. Code is not provided.", "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7d27\u5bc6\u8026\u5408\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\u548cLiDAR\u60ef\u6027SLAM\u7cfb\u7edfLIMOT\uff0c\u9002\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u3002LIMOT\u4f7f\u7528\u7531\u76ee\u6807\u68c0\u6d4b\u5668\u751f\u6210\u7684\u4e09\u7ef4\u8fb9\u754c\u6846\uff0c\u5e76\u4f7f\u7528IMU\u9884\u79ef\u5206\u8fdb\u884cLiDAR\u91cc\u7a0b\u8ba1\u8ba1\u7b97\u3002\u57fa\u4e8e\u8ddf\u8e2a\u5bf9\u8c61\u7684\u5386\u53f2\u8f68\u8ff9\uff0c\u6267\u884c\u9c81\u68d2\u7684\u5bf9\u8c61\u5173\u8054\u3002\u91c7\u7528\u57fa\u4e8e\u8f68\u8ff9\u7684\u52a8\u6001\u7279\u5f81\u8fc7\u6ee4\u65b9\u6cd5\u6765\u8fc7\u6ee4\u5c5e\u4e8e\u79fb\u52a8\u5bf9\u8c61\u7684\u7279\u5f81\u3002\u7136\u540e\u8fdb\u884c\u56e0\u5b50\u56fe\u4f18\u5316\uff0c\u4ee5\u4f18\u5316IMU\u504f\u5dee\u548c\u59ff\u6001\u4f30\u8ba1\u3002\u5b9e\u9a8c\u8868\u660e\uff0cLIMOT\u6bd4\u57fa\u51c6\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u59ff\u6001\u548c\u8ddf\u8e2a\u7cbe\u5ea6\u3002\u672a\u63d0\u4f9b\u4ee3\u7801\u3002"]}, {"id": "2305.00348", "title": "Modality-invariant Visual Odometry for Embodied Vision", "url": "http://arxiv.org/pdf/2305.00348.pdf", "time": "2023-04-29", "code": null, "context": ["This study proposes a Transformer-based modality-invariant Visual Odometry (VO) approach that can deal with diverse or changing sensor suites of navigation agents. The model outperforms previous methods while training on only a fraction of the data. This method opens the door to a broader range of real-world applications that can benefit from flexible and learned VO models. Code is not provided.", "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u6a21\u6001\u4e0d\u53d8Visual Odometry\uff08VO\uff09\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5904\u7406\u5bfc\u822a\u4ee3\u7406\u7684\u591a\u6837\u5316\u6216\u53d8\u5316\u7684\u4f20\u611f\u5668\u5957\u4ef6\u3002\u8be5\u6a21\u578b\u5728\u53ea\u4f7f\u7528\u90e8\u5206\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u4f18\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u4e3a\u66f4\u5e7f\u6cdb\u7684\u5b9e\u9645\u5e94\u7528\u5f00\u542f\u4e86\u5927\u95e8\uff0c\u53ef\u4ee5\u4ece\u7075\u6d3b\u548c\u5b66\u4e60\u5230\u7684VO\u6a21\u578b\u4e2d\u53d7\u76ca\u3002\u672a\u63d0\u4f9b\u4ee3\u7801\u3002"]}, {"id": "2304.14560", "title": "Neural Implicit Dense Semantic SLAM", "url": "http://arxiv.org/pdf/2304.14560.pdf", "time": "2023-04-27", "code": null, "context": ["This paper presents an efficient online framework for solving the semantic Visual Simultaneous Localization and Mapping (V-SLAM) problem for indoor scenes using neural implicit scene representation. The proposed method disentangles tracking and 3D mapping pipeline for computing robust and accurate camera motion. Dense and multifaceted scene representation is provided using neural fields for SDF, semantics, RGB, and depth. The set of keyframes is shown to be sufficient for learning excellent scene representation, thereby improving the pipeline\\'s train time. Multiple local mapping networks can be used to extend the pipeline for large-scale scenes. Extensive experiments show accurate tracking, mapping, and semantic labeling with noisy and sparse depth measurements. The pipeline can also easily extend to RGB image input. Code is not provided.", "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u795e\u7ecf\u9690\u5f0f\u573a\u666f\u8868\u793a\u6cd5\u89e3\u51b3\u8bed\u4e49\u89c6\u89c9\u540c\u65f6\u5b9a\u4f4d\u548c\u5730\u56fe\u6784\u5efa\uff08V-SLAM\uff09\u95ee\u9898\u7684\u9ad8\u6548\u5728\u7ebf\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u5ba4\u5185\u573a\u666f\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5c06\u8ddf\u8e2a\u548c3D\u6620\u5c04\u6d41\u6c34\u7ebf\u533a\u5206\u5f00\u6765\uff0c\u7528\u4e8e\u8ba1\u7b97\u7a33\u5065\u4e14\u51c6\u786e\u7684\u76f8\u673a\u8fd0\u52a8\u3002\u4f7f\u7528\u795e\u7ecf\u573a\u63d0\u4f9bSDF\u3001\u8bed\u4e49\u3001RGB\u548c\u6df1\u5ea6\u7684\u5bc6\u96c6\u591a\u9762\u573a\u666f\u8868\u793a\u3002\u901a\u8fc7\u5b9e\u9a8c\u8868\u660e\uff0c\u5173\u952e\u5e27\u96c6\u5408\u8db3\u4ee5\u5b66\u4e60\u51fa\u4f18\u79c0\u7684\u573a\u666f\u8868\u793a\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6d41\u6c34\u7ebf\u7684\u8bad\u7ec3\u65f6\u95f4\uff0c\u540c\u65f6\u53ef\u4ee5\u4f7f\u7528\u591a\u4e2a\u672c\u5730\u6620\u5c04\u7f51\u7edc\u6765\u6269\u5c55\u5927\u89c4\u6a21\u573a\u666f\u7684\u6d41\u6c34\u7ebf\u3002\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u53ef\u4ee5\u5728\u566a\u58f0\u548c\u7a00\u758f\u6df1\u5ea6\u6d4b\u91cf\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u51c6\u786e\u7684\u8ddf\u8e2a\u3001\u5730\u56fe\u6784\u5efa\u548c\u8bed\u4e49\u6807\u6ce8\u3002\u8be5\u6d41\u6c34\u7ebf\u4e5f\u53ef\u4ee5\u8f7b\u677e\u6269\u5c55\u5230RGB\u56fe\u50cf\u8f93\u5165\u3002\u672a\u63d0\u4f9b\u4ee3\u7801\u3002"]}, {"id": "2304.14377", "title": "Co-SLAM: Joint Coordinate and Sparse Parametric Encodings for Neural Real-Time SLAM", "url": "http://arxiv.org/pdf/2304.14377.pdf", "time": "2023-04-27", "code": null, "context": ["This paper presents Co-SLAM, a neural RGB-D SLAM system based on a hybrid representation that performs real-time and robust camera tracking and high-fidelity surface reconstruction. Co-SLAM uses a multi-resolution hash-grid representation and one-blob encoding to exploit high convergence speed and ability to represent high-frequency local features, encouraging surface coherence and completion in unobserved areas. The joint parametric-coordinate encoding brings the best of both worlds: fast convergence and surface hole filling. Co-SLAM also performs global bundle adjustment over all keyframes, allowing for state-of-the-art scene reconstruction results and competitive tracking performance in various datasets and benchmarks. Project page: https://hengyiwang.github.io/projects/CoSLAM. Code is not provided.", "\u672c\u6587\u63d0\u51fa\u4e86Co-SLAM\uff0c\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408\u8868\u793a\u7684\u795e\u7ecfRGB-D SLAM\u7cfb\u7edf\uff0c\u53ef\u5b9e\u73b0\u5b9e\u65f6\u548c\u7a33\u5065\u7684\u76f8\u673a\u8ddf\u8e2a\u548c\u9ad8\u4fdd\u771f\u5ea6\u8868\u9762\u91cd\u5efa\u3002Co-SLAM\u4f7f\u7528\u591a\u5206\u8fa8\u7387\u54c8\u5e0c\u7f51\u683c\u8868\u793a\u548c\u201cone-blob\u201d\u7f16\u7801\uff0c\u4ee5\u5229\u7528\u9ad8\u6536\u655b\u901f\u5ea6\u548c\u80fd\u591f\u8868\u793a\u9ad8\u9891\u5c40\u90e8\u7279\u5f81\u7684\u80fd\u529b\uff0c\u9f13\u52b1\u8868\u9762\u7684\u8fde\u7eed\u6027\u548c\u5b8c\u6210\u6027\u3002\u8054\u5408\u53c2\u6570\u5750\u6807\u7f16\u7801\u5c06\u5feb\u901f\u6536\u655b\u548c\u8868\u9762\u5b54\u586b\u5145\u7684\u4f18\u70b9\u7ed3\u5408\u8d77\u6765\u3002Co-SLAM\u8fd8\u5728\u6240\u6709\u5173\u952e\u5e27\u4e0a\u6267\u884c\u5168\u5c40\u675f\u8c03\u6574\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u573a\u666f\u91cd\u5efa\u7ed3\u679c\u548c\u5404\u79cd\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u7ade\u4e89\u8ddf\u8e2a\u6027\u80fd\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://hengyiwang.github.io/projects/CoSLAM\u3002\u672a\u63d0\u4f9b\u4ee3\u7801\u3002"]}], "NeRF": [{"id": "2305.01643", "title": "Neural LiDAR Fields for Novel View Synthesis", "url": "http://arxiv.org/pdf/2305.01643.pdf", "time": "2023-05-02", "code": null, "context": ["This paper presents Neural Fields for LiDAR (NFL), a method that optimizes a neural field scene representation from LiDAR measurements to synthesize realistic LiDAR scans from novel viewpoints. NFL combines the rendering power of neural fields with a physically motivated model of the LiDAR sensing process, accurately reproducing key sensor behaviors like beam divergence, secondary returns, and ray dropping. The method outperforms explicit reconstruct-then-simulate methods as well as other NeRF-style methods on LiDAR novel view synthesis tasks, as shown on both synthetic and real LiDAR scans. The improved realism of the synthesized views narrows the domain gap to real scans, resulting in better registration and semantic segmentation performance. Code is not provided.", "\u672c\u6587\u63d0\u51fa\u4e86\u7528\u4e8eLiDAR\u7684\u795e\u7ecf\u573a\uff08NFL\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4eceLiDAR\u6d4b\u91cf\u4e2d\u4f18\u5316\u795e\u7ecf\u573a\u573a\u666f\u8868\u793a\uff0c\u4ee5\u4ece\u65b0\u89c6\u70b9\u5408\u6210\u903c\u771f\u7684LiDAR\u626b\u63cf\u3002NFL\u5c06\u795e\u7ecf\u573a\u7684\u6e32\u67d3\u80fd\u529b\u4e0eLiDAR\u611f\u77e5\u8fc7\u7a0b\u7684\u8be6\u7ec6\u7269\u7406\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u53ef\u4ee5\u51c6\u786e\u5730\u518d\u73b0\u5173\u952e\u4f20\u611f\u5668\u884c\u4e3a\uff0c\u5982\u5149\u675f\u53d1\u6563\u3001\u4e8c\u6b21\u56de\u6ce2\u548c\u5149\u7ebf\u4e22\u5931\u3002\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u548c\u771f\u5b9eLiDAR\u626b\u63cf\u4e0a\u7684\u8868\u73b0\u5747\u4f18\u4e8e\u663e\u5f0f\u91cd\u5efa-\u6a21\u62df\u65b9\u6cd5\u4ee5\u53ca\u5176\u4ed6NeRF\u98ce\u683c\u7684LiDAR\u65b0\u89c6\u70b9\u5408\u6210\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bc1\u660e\u5408\u6210\u89c6\u56fe\u7684\u6539\u8fdb\u903c\u771f\u5ea6\u7f29\u5c0f\u4e86\u4e0e\u771f\u5b9e\u626b\u63cf\u4e4b\u95f4\u7684\u9886\u57df\u5dee\u8ddd\uff0c\u5e76\u8f6c\u5316\u4e3a\u66f4\u597d\u7684\u914d\u51c6\u548c\u8bed\u4e49\u5206\u5272\u6027\u80fd\u3002\u672a\u63d0\u4f9b\u4ee3\u7801\u3002"]}, {"id": "2305.01190", "title": "LatentAvatar: Learning Latent Expression Code for Expressive Neural Head Avatar", "url": "http://arxiv.org/pdf/2305.01190.pdf", "time": "2023-05-02", "code": null, "context": ["This paper presents LatentAvatar, an expressive neural head avatar driven by latent expression codes learned in an end-to-end and self-supervised manner without templates. Existing approaches to animatable NeRF-based head avatars are heavily bound by the expression power and tracking accuracy of templates. LatentAvatar leverages a latent head NeRF to learn person-specific latent expression codes from monocular portrait videos and uses a Y-shaped network to learn shared latent expression codes of different subjects for cross-identity reenactment. By optimizing photometric reconstruction objectives in NeRF, the learned latent expression codes are 3D-aware and capture high-frequency detailed expressions, enabling LatentAvatar to perform expressive reenactment between different subjects. Experimental results show that LatentAvatar outperforms previous state-of-the-art solutions in both quantitative and qualitative comparisons, capturing challenging expressions and subtle movements of teeth and eyeballs. Project page: https://www.liuyebin.com/latentavatar. Code is not provided.", "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8868\u60c5\u81ea\u9002\u5e94\u7684\u795e\u7ecf\u5934\u50cf\u751f\u6210\u65b9\u6cd5LatentAvatar\uff0c\u5176\u901a\u8fc7\u5b66\u4e60\u7aef\u5230\u7aef\u548c\u81ea\u76d1\u7763\u7684\u65b9\u5f0f\uff0c\u4f7f\u7528\u6f5c\u5728\u8868\u60c5\u7f16\u7801\u6765\u9a71\u52a8\u795e\u7ecf\u5143\u6e32\u67d3\u5f62\u5f0f\u7684\u5934\u50cf\u751f\u6210\u3002\u73b0\u6709\u7684\u57fa\u4e8eNeRF\u7684\u5934\u50cf\u751f\u6210\u65b9\u6cd5\u5f80\u5f80\u4f9d\u8d56\u4e8e\u9762\u90e8\u6a21\u677f\u6216\u4f7f\u7528\u8868\u60c5\u7cfb\u6570\u4f5c\u4e3a\u9a71\u52a8\u4fe1\u53f7\uff0c\u5176\u6027\u80fd\u53d7\u5230\u6a21\u677f\u8868\u60c5\u548c\u8ddf\u8e2a\u7cbe\u5ea6\u7684\u9650\u5236\u3002LatentAvatar\u5229\u7528\u6f5c\u5728\u5934\u90e8NeRF\u4ece\u5355\u76ee\u8096\u50cf\u89c6\u9891\u4e2d\u5b66\u4e60\u7279\u5b9a\u4e2a\u4f53\u7684\u6f5c\u5728\u8868\u60c5\u7f16\u7801\uff0c\u5e76\u4f7f\u7528Y\u5f62\u7f51\u7edc\u5b66\u4e60\u4e0d\u540c\u4e3b\u4f53\u7684\u5171\u4eab\u6f5c\u5728\u8868\u60c5\u7f16\u7801\uff0c\u4ee5\u5b9e\u73b0\u8de8\u8eab\u4efd\u91cd\u73b0\u3002\u901a\u8fc7\u5728NeRF\u4e2d\u4f18\u5316\u5149\u5ea6\u91cd\u5efa\u76ee\u6807\uff0c\u5b66\u4e60\u7684\u6f5c\u5728\u8868\u60c5\u7f16\u7801\u5177\u67093D\u611f\u77e5\u80fd\u529b\uff0c\u80fd\u591f\u6355\u6349\u9ad8\u9891\u7ec6\u8282\u8868\u60c5\uff0c\u4f7fLatentAvatar\u80fd\u591f\u5728\u4e0d\u540c\u4e3b\u4f53\u4e4b\u95f4\u8fdb\u884c\u8868\u60c5\u91cd\u73b0\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLatentAvatar\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u6bd4\u8f83\u4e2d\u5747\u4f18\u4e8e\u5148\u524d\u7684\u6700\u5148\u8fdb\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6355\u6349\u5177\u6709\u6311\u6218\u6027\u7684\u8868\u60c5\u548c\u7259\u9f7f\u3001\u773c\u7403\u7684\u5fae\u5c0f\u8fd0\u52a8\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://www.liuyebin.com/latentavatar\u3002\u672a\u63d0\u4f9b\u4ee3\u7801\u3002"]}, {"id": "2305.01163", "title": "Federated Neural Radiance Fields", "url": "http://arxiv.org/pdf/2305.01163.pdf", "time": "2023-05-02", "code": null, "context": ["This paper presents a federated learning approach to training neural radiance fields (NeRFs) for scene representation. Previous approaches have relied on centralized learning, assuming that all training images are available on one compute node. In contrast, this paper considers federated learning, where multiple compute nodes each having acquired a distinct set of observations learn a common NeRF in parallel. This supports the scenario of cooperatively modeling a scene using multiple agents. The contribution is the first federated learning algorithm for NeRF that splits the training effort across multiple compute nodes and avoids the need to pool images at a central node. A technique based on low-rank decomposition of NeRF layers is introduced to reduce bandwidth consumption for aggregating model parameters. Transferring compressed models instead of raw data also enhances the privacy of the data collecting agents. Code is not provided.", "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bad\u7ec3\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u8fdb\u884c\u573a\u666f\u8868\u793a\u3002\u5148\u524d\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u96c6\u4e2d\u5f0f\u5b66\u4e60\uff0c\u5047\u5b9a\u6240\u6709\u7684\u8bad\u7ec3\u56fe\u50cf\u90fd\u5728\u4e00\u4e2a\u8ba1\u7b97\u8282\u70b9\u4e0a\u53ef\u7528\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u672c\u6587\u8003\u8651\u4e86\u8054\u90a6\u5b66\u4e60\uff0c\u5176\u4e2d\u591a\u4e2a\u8ba1\u7b97\u8282\u70b9\u5206\u522b\u83b7\u53d6\u4e0d\u540c\u7684\u89c2\u5bdf\u6570\u636e\uff0c\u4ee5\u5e76\u884c\u65b9\u5f0f\u5b66\u4e60\u5171\u540c\u7684NeRF\u3002\u8fd9\u652f\u6301\u4e86\u4f7f\u7528\u591a\u4e2a\u4ee3\u7406\u534f\u540c\u5efa\u6a21\u573a\u666f\u7684\u60c5\u51b5\u3002\u672c\u6587\u7684\u8d21\u732e\u662f\u7b2c\u4e00\u4e2a\u9488\u5bf9NeRF\u7684\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\uff0c\u5b83\u5c06\u8bad\u7ec3\u5de5\u4f5c\u5206\u5e03\u5728\u591a\u4e2a\u8ba1\u7b97\u8282\u70b9\u4e0a\uff0c\u5e76\u907f\u514d\u4e86\u5728\u4e2d\u5fc3\u8282\u70b9\u6c47\u603b\u56fe\u50cf\u7684\u9700\u8981\u3002\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8eNeRF\u5c42\u7684\u4f4e\u79e9\u5206\u89e3\u6280\u672f\uff0c\u4ee5\u51cf\u5c11\u805a\u5408\u6a21\u578b\u53c2\u6570\u7684\u5e26\u5bbd\u6d88\u8017\u3002\u4e0e\u5176\u4f20\u8f93\u539f\u59cb\u6570\u636e\uff0c\u4f20\u8f93\u538b\u7f29\u6a21\u578b\u8fd8\u53ef\u4ee5\u63d0\u9ad8\u6570\u636e\u6536\u96c6\u4ee3\u7406\u7684\u9690\u79c1\u6027\u3002\u672a\u63d0\u4f9b\u4ee3\u7801\u3002"]}, {"id": "2305.00787", "title": "GeneFace++: Generalized and Stable Real-Time Audio-Driven 3D Talking Face Generation", "url": "http://arxiv.org/pdf/2305.00787.pdf", "time": "2023-05-01", "code": null, "context": ["This paper addresses the challenge of generating talking person portraits with arbitrary speech audio, which is crucial for digital human and metaverse applications. The goal is to achieve generalized audio-lip synchronization, good video quality, and high system efficiency. Neural radiance field (NeRF) has become a popular rendering technique in this field due to its high-fidelity and 3D-consistent talking face generation capabilities with a few-minute-long training video. However, there are still several challenges for NeRF-based methods, including lip synchronization, video quality, and system efficiency. This paper proposes GeneFace++ to address these challenges by utilizing pitch contour as an auxiliary feature and introducing a temporal loss, proposing a landmark locally linear embedding method to regulate outliers in the predicted motion sequence, and designing a computationally efficient NeRF-based motion-to-video renderer for fast training and real-time inference. GeneFace++ is the first NeRF-based method that achieves stable and real-time talking face generation with generalized audio-lip synchronization. Extensive experiments show that GeneFace++ outperforms state-of-the-art baselines in both subjective and objective evaluation. Video samples are available at https://genefaceplusplus.github.io. Code is not provided.", "\u672c\u6587\u89e3\u51b3\u4e86\u4f7f\u7528\u4efb\u610f\u8bed\u97f3\u97f3\u9891\u751f\u6210\u8bf4\u8bdd\u4eba\u5934\u50cf\u7684\u6311\u6218\uff0c\u8fd9\u5bf9\u6570\u5b57\u4eba\u548c\u5143\u5b87\u5b99\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u76ee\u6807\u662f\u5b9e\u73b0\u5e7f\u4e49\u97f3\u9891\u5634\u5507\u540c\u6b65\u3001\u826f\u597d\u7684\u89c6\u9891\u8d28\u91cf\u548c\u9ad8\u7cfb\u7edf\u6548\u7387\u3002\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u7531\u4e8e\u5177\u6709\u9ad8\u4fdd\u771f\u5ea6\u548c3D\u4e00\u81f4\u7684\u8bf4\u8bdd\u4eba\u8138\u751f\u6210\u80fd\u529b\u800c\u6210\u4e3a\u8be5\u9886\u57df\u7684\u6d41\u884c\u6e32\u67d3\u6280\u672f\uff0c\u4ec5\u9700\u51e0\u5206\u949f\u7684\u8bad\u7ec3\u89c6\u9891\u3002\u7136\u800c\uff0cNeRF\u65b9\u6cd5\u4ecd\u7136\u5b58\u5728\u4e00\u4e9b\u6311\u6218\uff0c\u5305\u62ec\u5634\u5507\u540c\u6b65\u3001\u89c6\u9891\u8d28\u91cf\u548c\u7cfb\u7edf\u6548\u7387\u3002\u672c\u6587\u63d0\u51fa\u4e86GeneFace++\u6765\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u901a\u8fc7\u5c06\u57fa\u9891\u8f6e\u5ed3\u4f5c\u4e3a\u8f85\u52a9\u7279\u5f81\u5e76\u5728\u9762\u90e8\u8fd0\u52a8\u9884\u6d4b\u8fc7\u7a0b\u4e2d\u5f15\u5165\u65f6\u95f4\u635f\u5931\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5730\u6807\u5c40\u90e8\u7ebf\u6027\u5d4c\u5165\u65b9\u6cd5\u6765\u8c03\u8282\u9884\u6d4b\u8fd0\u52a8\u5e8f\u5217\u4e2d\u7684\u5f02\u5e38\u503c\u4ee5\u907f\u514d\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8ba1\u7b97\u6548\u7387\u9ad8\u7684NeRF\u8fd0\u52a8\u5230\u89c6\u9891\u6e32\u67d3\u5668\u4ee5\u5b9e\u73b0\u5feb\u901f\u8bad\u7ec3\u548c\u5b9e\u65f6\u63a8\u7406\u3002GeneFace++\u662f\u7b2c\u4e00\u4e2a\u5b9e\u73b0\u7a33\u5b9a\u548c\u5b9e\u65f6\u7684NeRF\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5e7f\u4e49\u97f3\u9891\u5634\u5507\u540c\u6b65\u7684\u8bf4\u8bdd\u4eba\u8138\u751f\u6210\u3002\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGeneFace++\u5728\u4e3b\u89c2\u548c\u5ba2\u89c2\u8bc4\u4f30\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002\u89c6\u9891\u6837\u672c\u53ef\u5728https://genefaceplusplus.github.io\u4e0a\u83b7\u5f97\u3002\u672a\u63d0\u4f9b\u4ee3\u7801\u3002"]}, {"id": "2305.00375", "title": "Neural Radiance Fields (NeRFs): A Review and Some Recent Developments", "url": "http://arxiv.org/pdf/2305.00375.pdf", "time": "2023-04-30", "code": null, "context": ["Neural Radiance Field (NeRF) is a framework that represents a 3D scene using a fully connected neural network, known as the Multi-Layer Perception (MLP). It was introduced for the task of novel view synthesis and is capable of achieving state-of-the-art photorealistic image renderings from a given continuous viewpoint. NeRFs have become a popular field of research, as recent developments have expanded the performance and capabilities of the base framework. These developments include methods that require less images to train the model for view synthesis, as well as methods that are able to generate views from unconstrained and dynamic scene representations.", "\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u662f\u4e00\u79cd\u6846\u67b6\uff0c\u5b83\u4f7f\u7528\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\uff08\u79f0\u4e3a\u591a\u5c42\u611f\u77e5\u5668\uff0cMLP\uff09\u6765\u8868\u793a3D\u573a\u666f\u3002\u5b83\u88ab\u5f15\u5165\u7528\u4e8e\u65b0\u89c6\u89d2\u5408\u6210\u4efb\u52a1\uff0c\u5e76\u80fd\u591f\u4ece\u7ed9\u5b9a\u7684\u8fde\u7eed\u89c6\u89d2\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u903c\u771f\u56fe\u50cf\u6e32\u67d3\u3002\u968f\u7740\u6700\u8fd1\u7684\u53d1\u5c55\uff0cNeRF\u5df2\u6210\u4e3a\u7814\u7a76\u7684\u70ed\u95e8\u9886\u57df\uff0c\u8fd9\u4e9b\u53d1\u5c55\u6269\u5c55\u4e86\u57fa\u672c\u6846\u67b6\u7684\u6027\u80fd\u548c\u529f\u80fd\u3002\u8fd9\u4e9b\u53d1\u5c55\u5305\u62ec\u9700\u8981\u66f4\u5c11\u7684\u56fe\u50cf\u6765\u8bad\u7ec3\u89c6\u89d2\u5408\u6210\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u4ee5\u53ca\u80fd\u591f\u4ece\u65e0\u7ea6\u675f\u548c\u52a8\u6001\u573a\u666f\u8868\u793a\u751f\u6210\u89c6\u56fe\u7684\u65b9\u6cd5\u3002"]}]}